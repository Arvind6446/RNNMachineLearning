{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arvind6446/RNNMachineLearning/blob/main/hamlet_lstm_gru_nextword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a631344",
      "metadata": {
        "id": "8a631344"
      },
      "source": [
        "# Next-word Prediction with an LSTM (Hamlet)\n",
        "\n",
        "This notebook trains an LSTM language model on *Shakespeare's Hamlet* (NLTK Gutenberg corpus) using 2-gram and 3-gram training windows after lemmatization.\n",
        "\n",
        "Outputs:\n",
        "- `hamlet_nextword_best.keras` (best checkpoint by `val_loss`)\n",
        "- `hamlet_nextword.keras` (final saved model)\n",
        "- `tokenizer.json` (tokenizer configuration)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "213ee3de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213ee3de",
        "outputId": "ba6efe4b-b77e-4d92-c0ce-edfe6f903b64",
        "ExecuteTime": {
          "end_time": "2025-12-30T14:04:22.537226Z",
          "start_time": "2025-12-30T14:04:22.532073Z"
        }
      },
      "source": [
        "# NLTK downloads (run once)\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/arvindmehta/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/arvindmehta/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/arvindmehta/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/arvindmehta/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/arvindmehta/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e07defd9",
      "metadata": {
        "id": "e07defd9",
        "ExecuteTime": {
          "end_time": "2025-12-30T14:04:22.546072Z",
          "start_time": "2025-12-30T14:04:22.543445Z"
        }
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TerminateOnNaN\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "08499ccc",
      "metadata": {
        "id": "08499ccc",
        "ExecuteTime": {
          "end_time": "2025-12-30T14:04:22.551918Z",
          "start_time": "2025-12-30T14:04:22.549735Z"
        }
      },
      "source": [
        "# ---------------------------\n",
        "# Load Hamlet from NLTK Gutenberg\n",
        "# ---------------------------\n",
        "data = gutenberg.raw('shakespeare-hamlet.txt')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b92b6bee",
      "metadata": {
        "id": "b92b6bee",
        "ExecuteTime": {
          "end_time": "2025-12-30T14:04:22.571518Z",
          "start_time": "2025-12-30T14:04:22.554702Z"
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding,\n",
        "    GRU,                 # ✅ IMPORTANT\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    SpatialDropout1D\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    TerminateOnNaN,\n",
        "    TensorBoard\n",
        ")\n",
        "\n",
        "\n",
        "class AdvanceGRURNN:\n",
        "    \"\"\"\n",
        "    Next-word prediction model (word-level) using GRU.\n",
        "    This is your same pipeline, but the recurrent layers are GRU instead of LSTM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seed: int = 42, seq_len: int = 30, vocab_limit: int | None = 8000):\n",
        "        print(\"Initializing GRU RNN Model Training\")\n",
        "\n",
        "        self.seed = seed\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_limit = vocab_limit  # ✅ helps on small datasets (optional)\n",
        "\n",
        "        self.tokenizer = None\n",
        "        self.total_words = None\n",
        "\n",
        "        self.max_sequence_len = self.seq_len + 1\n",
        "        self.model = None\n",
        "        self.last_log_dir = None\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 1) DATASET I/O\n",
        "    # ---------------------------------------------------------------------\n",
        "    def loadDataSet(self, file_name: str = \"hamlet.txt\"):\n",
        "        \"\"\"\n",
        "        Expects a variable named `data` in notebook scope.\n",
        "        Example:\n",
        "            from nltk.corpus import gutenberg\n",
        "            data = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "        \"\"\"\n",
        "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(data)\n",
        "\n",
        "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        self.tokenizeDataSet(text)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 2) PREPROCESSING\n",
        "    # ---------------------------------------------------------------------\n",
        "    def _clean_line(self, line: str) -> str:\n",
        "        line = line.lower().strip()\n",
        "        line = re.sub(r\"[^a-z\\.\\,\\?\\!\\;\\:\\'\\-\\s]\", \" \", line)\n",
        "        line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "        return line\n",
        "\n",
        "    def _prepare_lines(self, text: str):\n",
        "        lines = []\n",
        "        for raw in text.split(\"\\n\"):\n",
        "            cleaned = self._clean_line(raw)\n",
        "            if cleaned:\n",
        "                lines.append(cleaned)\n",
        "        return lines\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 3) TOKENIZATION + TRAIN/VAL SPLIT\n",
        "    # ---------------------------------------------------------------------\n",
        "    def tokenizeDataSet(self, text: str):\n",
        "        lines = self._prepare_lines(text)\n",
        "        if len(lines) < 50:\n",
        "            raise ValueError(\"Not enough lines after cleaning to create train/val split.\")\n",
        "\n",
        "        split_idx = int(0.9 * len(lines))\n",
        "        train_lines = lines[:split_idx]\n",
        "        val_lines = lines[split_idx:]\n",
        "\n",
        "        # ✅ vocab_limit (optional) improves learning on small corpora\n",
        "        if self.vocab_limit is None:\n",
        "            self.tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "        else:\n",
        "            self.tokenizer = Tokenizer(num_words=self.vocab_limit, oov_token=\"<OOV>\")\n",
        "\n",
        "        self.tokenizer.fit_on_texts(train_lines)\n",
        "\n",
        "        raw_vocab = len(self.tokenizer.word_index) + 1\n",
        "        self.total_words = raw_vocab if self.vocab_limit is None else min(raw_vocab, self.vocab_limit)\n",
        "\n",
        "        print(\"Total words (vocab size):\", self.total_words)\n",
        "        print(\"Train lines:\", len(train_lines), \"| Val lines:\", len(val_lines))\n",
        "\n",
        "        train_seqs = self.createInputSequences(train_lines)\n",
        "        val_seqs = self.createInputSequences(val_lines)\n",
        "\n",
        "        print(f\"Train sequences: {len(train_seqs)} | Val sequences: {len(val_seqs)}\")\n",
        "\n",
        "        self.pad_and_train(train_seqs, val_seqs)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 4) SEQUENCE GENERATION\n",
        "    # ---------------------------------------------------------------------\n",
        "    def createInputSequences(self, lines):\n",
        "        seq_plus_label = self.seq_len + 1\n",
        "        sequences = []\n",
        "\n",
        "        for line in lines:\n",
        "            token_list = self.tokenizer.texts_to_sequences([line])[0]\n",
        "            if len(token_list) < 2:\n",
        "                continue\n",
        "\n",
        "            if len(token_list) <= seq_plus_label:\n",
        "                sequences.append(token_list)\n",
        "            else:\n",
        "                for i in range(seq_plus_label, len(token_list) + 1):\n",
        "                    sequences.append(token_list[i - seq_plus_label:i])\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def pad_and_train(self, train_sequences, val_sequences):\n",
        "        train_arr = np.array(\n",
        "            pad_sequences(train_sequences, maxlen=self.max_sequence_len, padding=\"pre\")\n",
        "        )\n",
        "        val_arr = np.array(\n",
        "            pad_sequences(val_sequences, maxlen=self.max_sequence_len, padding=\"pre\")\n",
        "        )\n",
        "\n",
        "        X_train = train_arr[:, :-1].astype(\"int32\")\n",
        "        y_train = train_arr[:, -1].astype(\"int32\")\n",
        "        X_val = val_arr[:, :-1].astype(\"int32\")\n",
        "        y_val = val_arr[:, -1].astype(\"int32\")\n",
        "\n",
        "        tr_keep = y_train != 0\n",
        "        va_keep = y_val != 0\n",
        "        X_train, y_train = X_train[tr_keep], y_train[tr_keep]\n",
        "        X_val, y_val = X_val[va_keep], y_val[va_keep]\n",
        "\n",
        "        print(\"Shapes -> X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "        print(\"Shapes -> X_val:  \", X_val.shape, \"y_val:\", y_val.shape)\n",
        "\n",
        "        hparams = self.suggest_hparams(n_train=len(y_train))\n",
        "\n",
        "        train_ds = (\n",
        "            tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "            .shuffle(20000, seed=self.seed, reshuffle_each_iteration=True)\n",
        "            .batch(hparams[\"batch_size\"])\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        "        )\n",
        "        val_ds = (\n",
        "            tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "            .batch(hparams[\"batch_size\"])\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "        callbacks, log_dir = self.build_callbacks(\n",
        "            es_patience=hparams[\"es_patience\"],\n",
        "            checkpoint_path=\"hamlet_nextword_best.keras\",\n",
        "            log_root=\"logs\"\n",
        "        )\n",
        "        self.last_log_dir = log_dir\n",
        "\n",
        "        self.build_and_train(\n",
        "            train_ds=train_ds,\n",
        "            val_ds=val_ds,\n",
        "            embed_dim=hparams[\"embed_dim\"],\n",
        "            gru_units=hparams[\"gru_units\"],\n",
        "            dropout_rate=hparams[\"dropout_rate\"],\n",
        "            epochs=hparams[\"epochs\"],\n",
        "            initial_lr=hparams[\"initial_lr\"],\n",
        "            decay_steps=hparams[\"decay_steps\"],\n",
        "            decay_rate=hparams[\"decay_rate\"],\n",
        "            clipnorm=hparams[\"clipnorm\"],\n",
        "            callbacks=callbacks,\n",
        "            recurrent_dropout=hparams[\"recurrent_dropout\"],\n",
        "        )\n",
        "\n",
        "        self.save_model_keras(\n",
        "            keras_path=\"hamlet_nextword.keras\",\n",
        "            tokenizer_path=\"tokenizer.json\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 5) Hyperparameters\n",
        "    # ---------------------------------------------------------------------\n",
        "    def suggest_hparams(self, n_train: int):\n",
        "        embed_dim = 128\n",
        "        gru_units = 256\n",
        "\n",
        "        dropout_rate = 0.30 if n_train < 80_000 else 0.20\n",
        "        batch_size = 128 if n_train < 120_000 else 256\n",
        "\n",
        "        epochs = 80\n",
        "        es_patience = 8\n",
        "\n",
        "        initial_lr = 2e-3\n",
        "        decay_steps = max(1000, n_train // max(batch_size, 1))\n",
        "        decay_rate = 0.5\n",
        "\n",
        "        # ✅ recurrent_dropout often hurts speed; set to 0.0 if slow\n",
        "        recurrent_dropout = 0.0\n",
        "\n",
        "        return {\n",
        "            \"embed_dim\": embed_dim,\n",
        "            \"gru_units\": gru_units,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"epochs\": epochs,\n",
        "            \"es_patience\": es_patience,\n",
        "            \"initial_lr\": initial_lr,\n",
        "            \"decay_steps\": decay_steps,\n",
        "            \"decay_rate\": decay_rate,\n",
        "            \"clipnorm\": 1.0,\n",
        "            \"recurrent_dropout\": recurrent_dropout,\n",
        "        }\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 6) Optimizer (LR schedule)\n",
        "    # ---------------------------------------------------------------------\n",
        "    def make_optimizer(self, initial_lr, decay_steps, decay_rate, clipnorm=1.0):\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=decay_steps,\n",
        "            decay_rate=decay_rate,\n",
        "            staircase=True\n",
        "        )\n",
        "        return tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=clipnorm)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 7) Callbacks\n",
        "    # ---------------------------------------------------------------------\n",
        "    def build_callbacks(self, es_patience: int, checkpoint_path: str, log_root: str):\n",
        "        run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        log_dir = os.path.join(log_root, \"hamlet_nextword_gru\", run_id)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        callbacks = [\n",
        "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(filepath=checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "            EarlyStopping(monitor=\"val_loss\", patience=es_patience, restore_best_weights=True, verbose=1),\n",
        "            TerminateOnNaN(),\n",
        "        ]\n",
        "\n",
        "        print(f\"✅ TensorBoard logs: {log_dir}\")\n",
        "        return callbacks, log_dir\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 8) Build + Train (GRU)\n",
        "    # ---------------------------------------------------------------------\n",
        "    def build_and_train(\n",
        "        self,\n",
        "        train_ds,\n",
        "        val_ds,\n",
        "        embed_dim=128,\n",
        "        gru_units=256,\n",
        "        dropout_rate=0.3,\n",
        "        epochs=30,\n",
        "        initial_lr=2e-3,\n",
        "        decay_steps=1000,\n",
        "        decay_rate=0.5,\n",
        "        clipnorm=1.0,\n",
        "        callbacks=None,\n",
        "        recurrent_dropout=0.0\n",
        "    ):\n",
        "        model = Sequential([\n",
        "            Embedding(self.total_words, embed_dim, input_length=self.seq_len),\n",
        "            SpatialDropout1D(0.2),\n",
        "\n",
        "            GRU(gru_units, return_sequences=True, recurrent_dropout=recurrent_dropout),\n",
        "            Dropout(dropout_rate),\n",
        "\n",
        "            GRU(max(64, gru_units // 2), recurrent_dropout=recurrent_dropout),\n",
        "            Dropout(dropout_rate),\n",
        "\n",
        "            Dense(self.total_words, activation=\"softmax\"),\n",
        "        ])\n",
        "\n",
        "        optimizer = self.make_optimizer(initial_lr, decay_steps, decay_rate, clipnorm=clipnorm)\n",
        "\n",
        "        model.compile(\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            optimizer=optimizer,\n",
        "            metrics=[\n",
        "                tf.keras.metrics.SparseCategoricalAccuracy(name=\"top1\"),\n",
        "                tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5\"),\n",
        "                tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name=\"top10\"),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        model.build(input_shape=(None, self.seq_len))\n",
        "        self.model = model\n",
        "        print(self.model.summary())\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=epochs,\n",
        "            callbacks=(callbacks or []),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        best_val_loss = float(np.min(history.history.get(\"val_loss\", [np.nan])))\n",
        "        if not np.isnan(best_val_loss):\n",
        "            print(f\"Best val_loss: {best_val_loss:.4f} | Perplexity ≈ {math.exp(best_val_loss):.2f}\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 9) Save model + tokenizer\n",
        "    # ---------------------------------------------------------------------\n",
        "    def save_model_keras(self, keras_path: str, tokenizer_path: str):\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise RuntimeError(\"Model is not trained yet. Train before saving.\")\n",
        "\n",
        "        self.model.save(keras_path)\n",
        "        print(f\"✅ Saved Keras model: {keras_path}\")\n",
        "\n",
        "        with open(tokenizer_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(self.tokenizer.to_json())\n",
        "        print(f\"✅ Saved tokenizer: {tokenizer_path}\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 10) Generation (top-k sampling)\n",
        "    # ---------------------------------------------------------------------\n",
        "    def generate_text(self, seed_text: str, next_words: int = 20, temperature: float = 1.0, top_k: int = 20):\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise RuntimeError(\"Train the model first before generating text.\")\n",
        "\n",
        "        def sample_top_k(probs, k, temp):\n",
        "            probs = np.asarray(probs).astype(\"float64\")\n",
        "            probs = np.log(probs + 1e-12) / max(temp, 1e-6)\n",
        "            probs = np.exp(probs)\n",
        "            probs = probs / np.sum(probs)\n",
        "\n",
        "            if 0 < k < len(probs):\n",
        "                top_idx = np.argpartition(probs, -k)[-k:]\n",
        "                top_probs = probs[top_idx] / np.sum(probs[top_idx])\n",
        "                return int(np.random.choice(top_idx, p=top_probs))\n",
        "\n",
        "            return int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "        seed_clean = self._clean_line(seed_text)\n",
        "\n",
        "        for _ in range(next_words):\n",
        "            token_list = self.tokenizer.texts_to_sequences([seed_clean])[0]\n",
        "            token_list = pad_sequences([token_list], maxlen=self.seq_len, padding=\"pre\")\n",
        "            preds = self.model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "            next_index = sample_top_k(preds, top_k, temperature)\n",
        "            next_word = self.tokenizer.index_word.get(next_index, \"\")\n",
        "\n",
        "            if not next_word:\n",
        "                break\n",
        "\n",
        "            seed_clean += \" \" + next_word\n",
        "\n",
        "        return seed_clean\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "34f6ef9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "34f6ef9e",
        "outputId": "5888a499-3cac-48ba-d605-4e954828bc4b",
        "ExecuteTime": {
          "end_time": "2025-12-30T14:04:30.021653Z",
          "start_time": "2025-12-30T14:04:22.576390Z"
        }
      },
      "source": [
        "# ---------------------------\n",
        "# Run training\n",
        "# ---------------------------\n",
        "lstm = AdvanceLSTMRNN(seed=42, seq_len=30)\n",
        "lstm.loadDataSet()\n",
        "\n",
        "print(lstm.generate_text(\"to be or not to be\", next_words=30, temperature=0.9, top_k=30))\n",
        "\n",
        "# Example generation after training:\n",
        "# print(lstm.generate_text(\"to be or not to be\", next_words=30, temperature=0.8))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing LSTM RNN Model Training\n",
            "Total words (vocab size): 4512\n",
            "Train lines: 3569 | Val lines: 397\n",
            "Train sequences: 3497 | Val sequences: 385\n",
            "Shapes -> X_train: (3497, 30) y_train: (3497,)\n",
            "Shapes -> X_val:   (385, 30) y_val: (385,)\n",
            "✅ TensorBoard logs: logs/hamlet_nextword/20251230-193422\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m97\u001b[0m)         │       \u001b[38;5;34m437,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m97\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_12 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m160\u001b[0m)        │       \u001b[38;5;34m124,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m160\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_13 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │        \u001b[38;5;34m58,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4512\u001b[0m)           │       \u001b[38;5;34m365,472\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">437,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">124,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">365,472</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m985,536\u001b[0m (3.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">985,536</span> (3.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m985,536\u001b[0m (3.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">985,536</span> (3.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/100\n",
            "\u001b[1m 2/55\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 2s/step - loss: 8.4141 - top1: 0.0000e+00 - top10: 0.0039 - top5: 0.0039        "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m lstm = AdvanceLSTMRNN(seed=\u001b[32m42\u001b[39m, seq_len=\u001b[32m30\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadDataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(lstm.generate_text(\u001b[33m\"\u001b[39m\u001b[33mto be or not to be\u001b[39m\u001b[33m\"\u001b[39m, next_words=\u001b[32m30\u001b[39m, temperature=\u001b[32m0.9\u001b[39m, top_k=\u001b[32m30\u001b[39m))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Example generation after training:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(lstm.generate_text(\"to be or not to be\", next_words=30, temperature=0.8))\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mAdvanceLSTMRNN.loadDataSet\u001b[39m\u001b[34m(self, file_name)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     98\u001b[39m     text = f.read()\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizeDataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mAdvanceLSTMRNN.tokenizeDataSet\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_sequences_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_sequences_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Pad and train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mAdvanceLSTMRNN.pad_and_train\u001b[39m\u001b[34m(self, train_sequences, val_sequences)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.last_log_dir = log_dir\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_and_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membed_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlstm_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlstm_units\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdropout_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitial_lr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecay_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecay_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclipnorm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Save final model + tokenizer\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m.save_model_keras(\n\u001b[32m    266\u001b[39m     keras_path=\u001b[33m\"\u001b[39m\u001b[33mhamlet_nextword.keras\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    267\u001b[39m     save_tokenizer=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    268\u001b[39m     tokenizer_path=\u001b[33m\"\u001b[39m\u001b[33mtokenizer.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    269\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 465\u001b[39m, in \u001b[36mAdvanceLSTMRNN.build_and_train\u001b[39m\u001b[34m(self, train_ds, val_ds, embed_dim, lstm_units, dropout_rate, epochs, initial_lr, decay_steps, decay_rate, clipnorm, callbacks)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m    463\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.summary())\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(np.min(history.history.get(\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, [np.nan])))\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isnan(best_val_loss):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/PythonProject2/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/hamlet_nextword\n"
      ],
      "metadata": {
        "id": "VAmWzXg-Z8nM"
      },
      "id": "VAmWzXg-Z8nM",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}