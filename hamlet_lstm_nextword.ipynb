{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arvind6446/RNNMachineLearning/blob/main/hamlet_lstm_nextword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a631344",
      "metadata": {
        "id": "8a631344"
      },
      "source": [
        "# Next-word Prediction with an LSTM (Hamlet)\n",
        "\n",
        "This notebook trains an LSTM language model on *Shakespeare's Hamlet* (NLTK Gutenberg corpus) using 2-gram and 3-gram training windows after lemmatization.\n",
        "\n",
        "Outputs:\n",
        "- `hamlet_nextword_best.keras` (best checkpoint by `val_loss`)\n",
        "- `hamlet_nextword.keras` (final saved model)\n",
        "- `tokenizer.json` (tokenizer configuration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "213ee3de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213ee3de",
        "outputId": "50d9b6a0-a17a-44be-89aa-ebee197d882e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# NLTK downloads (run once)\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e07defd9",
      "metadata": {
        "id": "e07defd9"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TerminateOnNaN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08499ccc",
      "metadata": {
        "id": "08499ccc"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Load Hamlet from NLTK Gutenberg\n",
        "# ---------------------------\n",
        "data = gutenberg.raw('shakespeare-hamlet.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b92b6bee",
      "metadata": {
        "id": "b92b6bee"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdvanceLSTMRNN:\n",
        "    def __init__(self, seed: int = 42):\n",
        "        print(\"Initializing LSTM RNN Model Training\")\n",
        "        self.total_words = None\n",
        "        self.tokenizer = None\n",
        "        self.max_sequence_len = None\n",
        "        self.model = None\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.n_sequences = 0\n",
        "\n",
        "        # Reproducibility\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Dataset I/O\n",
        "    # ---------------------------\n",
        "    def loadDataSet(self):\n",
        "        with open('hamlet.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(data)\n",
        "\n",
        "        with open('hamlet.txt', 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        self.tokenizeDataSet(text)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Preprocessing\n",
        "    # ---------------------------\n",
        "    def _lemmatize_line(self, line: str) -> str:\n",
        "        tokens = word_tokenize(line.lower())\n",
        "        lemmas = [self.lemmatizer.lemmatize(t) for t in tokens if t.isalpha()]\n",
        "        return \" \".join(lemmas)\n",
        "\n",
        "    def tokenizeDataSet(self, text: str):\n",
        "        # Lemmatize per line\n",
        "        lemmatized_lines = []\n",
        "        for line in text.split('\\n'):\n",
        "            cleaned = self._lemmatize_line(line)\n",
        "            if cleaned.strip():\n",
        "                lemmatized_lines.append(cleaned)\n",
        "\n",
        "        # Tokenizer with OOV handling\n",
        "        self.tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "        self.tokenizer.fit_on_texts(lemmatized_lines)\n",
        "        self.total_words = len(self.tokenizer.word_index) + 1\n",
        "        print(\"Total words (lemmatized): \", self.total_words)\n",
        "\n",
        "        # Build 2-gram and 3-gram sequences\n",
        "        input_sequences = self.createInputSequences(lemmatized_lines)\n",
        "\n",
        "        # Pad and create predictors/labels\n",
        "        self.padSequences(input_sequences)\n",
        "\n",
        "    def createInputSequences(self, lines):\n",
        "        input_sequences = []\n",
        "        for line in lines:\n",
        "            token_list = self.tokenizer.texts_to_sequences([line])[0]\n",
        "            if len(token_list) < 2:\n",
        "                continue\n",
        "            # exact 2-gram and 3-gram windows\n",
        "            for n in (2, 3):\n",
        "                if len(token_list) >= n:\n",
        "                    for i in range(n, len(token_list) + 1):\n",
        "                        ngram = token_list[i - n:i]\n",
        "                        input_sequences.append(ngram)\n",
        "        self.n_sequences = len(input_sequences)\n",
        "        print(\"Total sequences (2&3-gram): \", self.n_sequences)\n",
        "        return input_sequences\n",
        "\n",
        "    def padSequences(self, input_sequences):\n",
        "        # Max length is 3 by design (2 or 3 grams)\n",
        "        self.max_sequence_len = max(len(x) for x in input_sequences)  # should be 3\n",
        "        input_sequences = np.array(\n",
        "            pad_sequences(input_sequences, maxlen=self.max_sequence_len, padding='pre')\n",
        "        )\n",
        "        self.createPredictorandLabels(input_sequences)\n",
        "\n",
        "    def createPredictorandLabels(self, input_sequences):\n",
        "        x, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "        y = to_categorical(y, num_classes=self.total_words)\n",
        "\n",
        "        x_train, x_val, y_train, y_val = train_test_split(\n",
        "            x, y, test_size=0.1, random_state=42, shuffle=True\n",
        "        )\n",
        "        print(\"Shapes -> X_train:\", x_train.shape, \"y_train:\", y_train.shape)\n",
        "\n",
        "        # Dynamic hyperparameters based on vocab size & dataset size\n",
        "        hparams = self.suggest_hparams()\n",
        "\n",
        "        # Callbacks: configure EarlyStopping on/off and patience\n",
        "        callbacks = self.build_callbacks(\n",
        "            use_early_stopping=True,         # set False to disable EarlyStopping\n",
        "            es_patience=hparams['es_patience'],\n",
        "            checkpoint_path=\"hamlet_nextword_best.keras\"  # Keras-native checkpoint\n",
        "        )\n",
        "\n",
        "        # Build & train LSTM\n",
        "        self.build_and_train(x_train, y_train, x_val, y_val,\n",
        "                             embed_dim=hparams['embed_dim'],\n",
        "                             lstm_units=hparams['lstm_units'],\n",
        "                             dropout_rate=hparams['dropout_rate'],\n",
        "                             epochs=hparams['epochs'],\n",
        "                             batch_size=hparams['batch_size'],\n",
        "                             initial_lr=hparams['initial_lr'],\n",
        "                             decay_steps=hparams['decay_steps'],\n",
        "                             decay_rate=hparams['decay_rate'],\n",
        "                             clipnorm=hparams['clipnorm'],\n",
        "                             callbacks=callbacks)\n",
        "\n",
        "        # Save trained model in Keras format and tokenizer\n",
        "        self.save_model_keras(\n",
        "            keras_path=\"hamlet_nextword.keras\",\n",
        "            save_tokenizer=True,\n",
        "            tokenizer_path=\"tokenizer.json\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------\n",
        "    # Dynamic hyperparameters\n",
        "    # ---------------------------\n",
        "    def suggest_hparams(self):\n",
        "        V = self.total_words\n",
        "        N = self.n_sequences\n",
        "\n",
        "        # Embedding dimension: sublinear in vocabulary size\n",
        "        embed_dim = int(np.clip(round(6 * math.log2(max(V, 4))), 64, 256))\n",
        "        # LSTM units: slightly higher than embed_dim\n",
        "        lstm_units = int(np.clip(round(12 * math.log2(max(V, 4))), 128, 384))\n",
        "        # Dropout depends on dataset size\n",
        "        dropout_rate = 0.15 if N > 200_000 else 0.25\n",
        "        # Batch size scales with dataset size\n",
        "        batch_size = 64 if N < 50_000 else (128 if N < 200_000 else 256)\n",
        "        # Epochs: fewer for larger datasets\n",
        "        epochs = 20 if N < 50_000 else (15 if N < 200_000 else 10)\n",
        "        # Learning rate schedule\n",
        "        initial_lr = 3e-3 if N < 50_000 else 2e-3\n",
        "        decay_steps = max(1000, N // max(batch_size, 1))\n",
        "        decay_rate = 0.5\n",
        "        # EarlyStopping patience\n",
        "        es_patience = 4 if epochs <= 15 else 6\n",
        "\n",
        "        return {\n",
        "            'embed_dim': embed_dim,\n",
        "            'lstm_units': lstm_units,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'batch_size': batch_size,\n",
        "            'epochs': epochs,\n",
        "            'initial_lr': initial_lr,\n",
        "            'decay_steps': decay_steps,\n",
        "            'decay_rate': decay_rate,\n",
        "            'clipnorm': 1.0,\n",
        "            'es_patience': es_patience,\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Optimizer & Callbacks\n",
        "    # ---------------------------\n",
        "    def make_optimizer(self, initial_lr, decay_steps, decay_rate, clipnorm=1.0):\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=decay_steps,\n",
        "            decay_rate=decay_rate,\n",
        "            staircase=True\n",
        "        )\n",
        "        return tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=clipnorm)\n",
        "\n",
        "    def build_callbacks(self, use_early_stopping: bool = True, es_patience: int = 4,\n",
        "                        checkpoint_path: str = \"best_model.keras\"):\n",
        "        callbacks = [\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=5e-5, verbose=1),\n",
        "            ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss',\n",
        "                            save_best_only=True, save_weights_only=False, verbose=1),\n",
        "            TerminateOnNaN()\n",
        "        ]\n",
        "        if use_early_stopping:\n",
        "            callbacks.append(EarlyStopping(monitor='val_loss', patience=es_patience,\n",
        "                                           restore_best_weights=True, verbose=1))\n",
        "        return callbacks\n",
        "\n",
        "    # ---------------------------\n",
        "    # Build & Train\n",
        "    # ---------------------------\n",
        "    def build_and_train(self, X_train, y_train, X_val, y_val,\n",
        "                        embed_dim=128, lstm_units=256, dropout_rate=0.2,\n",
        "                        epochs=15, batch_size=256,\n",
        "                        initial_lr=0.001, decay_steps=1000, decay_rate=0.5,\n",
        "                        clipnorm=1.0, callbacks=None):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.total_words, embed_dim, input_length=self.max_sequence_len - 1))\n",
        "        model.add(LSTM(lstm_units))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(self.total_words, activation='softmax'))\n",
        "\n",
        "        optimizer = self.make_optimizer(initial_lr, decay_steps, decay_rate, clipnorm=clipnorm)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "        print(self.model.summary())\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs, batch_size=batch_size,\n",
        "            callbacks=(callbacks or []),\n",
        "            verbose=1\n",
        "        )\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "        # Report the best validation loss and perplexity\n",
        "        import math as _math\n",
        "        best_val_loss = min(history.history.get('val_loss', [np.nan]))\n",
        "        if not np.isnan(best_val_loss):\n",
        "            print(f\"Best val_loss: {best_val_loss:.4f} | Perplexity ≈ {_math.exp(best_val_loss):.2f}\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Save model in .keras + tokenizer\n",
        "    # ---------------------------\n",
        "    def save_model_keras(self, keras_path: str = \"hamlet_nextword.keras\",\n",
        "                         save_tokenizer: bool = True,\n",
        "                         tokenizer_path: str = \"tokenizer.json\"):\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not trained yet. Train before saving.\")\n",
        "\n",
        "        self.model.save(keras_path)\n",
        "        print(f\"✅ Saved Keras model: {keras_path}\")\n",
        "\n",
        "        if save_tokenizer and self.tokenizer is not None:\n",
        "            tok_json = self.tokenizer.to_json()\n",
        "            with open(tokenizer_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(tok_json)\n",
        "            print(f\"✅ Saved tokenizer: {tokenizer_path}\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Simple generation helper\n",
        "    # ---------------------------\n",
        "    def generate_text(self, seed_text: str, next_words: int = 20, temperature: float = 1.0):\n",
        "        def sample_with_temperature(preds, temp):\n",
        "            preds = np.asarray(preds).astype('float64')\n",
        "            preds = np.log(preds + 1e-8) / max(temp, 1e-6)\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            return np.argmax(np.random.multinomial(1, preds, 1))\n",
        "\n",
        "        seed_lemmatized = self._lemmatize_line(seed_text)\n",
        "        for _ in range(next_words):\n",
        "            token_list = self.tokenizer.texts_to_sequences([seed_lemmatized])[0]\n",
        "            token_list = pad_sequences([token_list], maxlen=self.max_sequence_len - 1, padding='pre')\n",
        "            preds = self.model.predict(token_list, verbose=0)[0]\n",
        "            next_index = (sample_with_temperature(preds, temperature) if temperature and temperature != 1.0\n",
        "                          else np.argmax(preds))\n",
        "            next_word = self.tokenizer.index_word.get(next_index, '')\n",
        "            if not next_word:\n",
        "                break\n",
        "            seed_lemmatized += \" \" + next_word\n",
        "        return seed_lemmatized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "34f6ef9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "34f6ef9e",
        "outputId": "5b2adac8-d003-4e65-a98d-1dac7d9316de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing LSTM RNN Model Training\n",
            "Total words (lemmatized):  4304\n",
            "Total sequences (2&3-gram):  46867\n",
            "Shapes -> X_train: (42180, 2) y_train: (42180, 4304)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0369 - loss: 6.8178\n",
            "Epoch 1: val_loss improved from inf to 6.28359, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.0370 - loss: 6.8164 - val_accuracy: 0.0544 - val_loss: 6.2836 - learning_rate: 0.0030\n",
            "Epoch 2/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0658 - loss: 5.9956\n",
            "Epoch 2: val_loss improved from 6.28359 to 6.05768, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.0658 - loss: 5.9954 - val_accuracy: 0.0804 - val_loss: 6.0577 - learning_rate: 0.0015\n",
            "Epoch 3/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0894 - loss: 5.6514\n",
            "Epoch 3: val_loss improved from 6.05768 to 5.96265, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.0894 - loss: 5.6512 - val_accuracy: 0.0945 - val_loss: 5.9627 - learning_rate: 0.0015\n",
            "Epoch 4/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1085 - loss: 5.4224\n",
            "Epoch 4: val_loss improved from 5.96265 to 5.91706, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.1085 - loss: 5.4222 - val_accuracy: 0.1031 - val_loss: 5.9171 - learning_rate: 7.5000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1254 - loss: 5.2594\n",
            "Epoch 5: val_loss improved from 5.91706 to 5.85771, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.1254 - loss: 5.2594 - val_accuracy: 0.1086 - val_loss: 5.8577 - learning_rate: 3.7500e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1387 - loss: 5.1140\n",
            "Epoch 6: val_loss improved from 5.85771 to 5.84213, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.1387 - loss: 5.1139 - val_accuracy: 0.1139 - val_loss: 5.8421 - learning_rate: 3.7500e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1474 - loss: 5.0341\n",
            "Epoch 7: val_loss improved from 5.84213 to 5.81024, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.1474 - loss: 5.0341 - val_accuracy: 0.1156 - val_loss: 5.8102 - learning_rate: 1.8750e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1519 - loss: 4.9775\n",
            "Epoch 8: val_loss improved from 5.81024 to 5.78811, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.1519 - loss: 4.9775 - val_accuracy: 0.1178 - val_loss: 5.7881 - learning_rate: 9.3750e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1551 - loss: 4.9371\n",
            "Epoch 9: val_loss improved from 5.78811 to 5.78611, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.1551 - loss: 4.9370 - val_accuracy: 0.1186 - val_loss: 5.7861 - learning_rate: 9.3750e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1571 - loss: 4.9215\n",
            "Epoch 10: val_loss improved from 5.78611 to 5.78158, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.1571 - loss: 4.9215 - val_accuracy: 0.1205 - val_loss: 5.7816 - learning_rate: 4.6875e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1597 - loss: 4.9016\n",
            "Epoch 11: val_loss improved from 5.78158 to 5.77715, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.1597 - loss: 4.9015 - val_accuracy: 0.1205 - val_loss: 5.7771 - learning_rate: 2.3438e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1591 - loss: 4.8921\n",
            "Epoch 12: val_loss improved from 5.77715 to 5.77678, saving model to hamlet_nextword_best.keras\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.1591 - loss: 4.8921 - val_accuracy: 0.1210 - val_loss: 5.7768 - learning_rate: 2.3438e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m658/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1577 - loss: 4.8887\n",
            "Epoch 13: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.1577 - loss: 4.8886 - val_accuracy: 0.1210 - val_loss: 5.7782 - learning_rate: 1.1719e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1605 - loss: 4.8804\n",
            "Epoch 14: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.1605 - loss: 4.8804 - val_accuracy: 0.1210 - val_loss: 5.7786 - learning_rate: 5.8594e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m659/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1616 - loss: 4.8864\n",
            "Epoch 15: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.1616 - loss: 4.8863 - val_accuracy: 0.1210 - val_loss: 5.7781 - learning_rate: 5.8594e-06\n",
            "Epoch 16/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1618 - loss: 4.8794\n",
            "Epoch 16: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.1618 - loss: 4.8794 - val_accuracy: 0.1210 - val_loss: 5.7791 - learning_rate: 2.9297e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1615 - loss: 4.8745\n",
            "Epoch 17: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.1615 - loss: 4.8745 - val_accuracy: 0.1210 - val_loss: 5.7792 - learning_rate: 1.4648e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1615 - loss: 4.8721\n",
            "Epoch 18: val_loss did not improve from 5.77678\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.1615 - loss: 4.8721 - val_accuracy: 0.1210 - val_loss: 5.7790 - learning_rate: 1.4648e-06\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "Training complete.\n",
            "Best val_loss: 5.7768 | Perplexity ≈ 322.72\n",
            "✅ Saved Keras model: hamlet_nextword.keras\n",
            "✅ Saved tokenizer: tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Run training\n",
        "# ---------------------------\n",
        "lstm = AdvanceLSTMRNN()\n",
        "lstm.loadDataSet()\n",
        "\n",
        "# Example generation after training:\n",
        "# print(lstm.generate_text(\"to be or not to be\", next_words=30, temperature=0.8))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}